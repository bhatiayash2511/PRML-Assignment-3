{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Instructions to students:\n",
    "\n",
    "1. There are 5 types of cells in this notebook. The cell type will be indicated within the cell.\n",
    "    1. Markdown cells with problem written in it. (DO NOT TOUCH THESE CELLS) (**Cell type: TextRead**)\n",
    "    2. Python cells with setup code for further evaluations. (DO NOT TOUCH THESE CELLS) (**Cell type: CodeRead**)\n",
    "    3. Python code cells with some template code or empty cell. (FILL CODE IN THESE CELLS BASED ON INSTRUCTIONS IN CURRENT AND PREVIOUS CELLS) (**Cell type: CodeWrite**)\n",
    "    4. Markdown cells where a written reasoning or conclusion is expected. (WRITE SENTENCES IN THESE CELLS) (**Cell type: TextWrite**)\n",
    "    5. Temporary code cells for convenience and TAs. (YOU MAY DO WHAT YOU WILL WITH THESE CELLS, TAs WILL REPLACE WHATEVER YOU WRITE HERE WITH OFFICIAL EVALUATION CODE) (**Cell type: Convenience**)\n",
    "    \n",
    "2. You are not allowed to insert new cells in the submitted notebook.\n",
    "\n",
    "3. You are not allowed to import any extra packages.\n",
    "\n",
    "4. The code is to be written in Python 3.6 or later syntax. Latest versions of other packages maybe assumed.\n",
    "\n",
    "5. In CodeWrite Cells, the only outputs to be given are plots asked in the question. Nothing else to be output/print. \n",
    "\n",
    "6. If TextWrite cells ask you to give accuracy/error/other numbers you can print them on the code cells, but remove the print statements before submitting.\n",
    "\n",
    "7. The convenience code can be used to check the expected syntax of the functions. At a minimum, your entire notebook must run with \"run all\" with the convenience cells as it is. Any runtime failures on the submitted notebook as it is will get zero marks.\n",
    "\n",
    "8. All code must be written by yourself. Copying from other students/material on the web is strictly prohibited. Any violations will result in zero marks.\n",
    "\n",
    "9. All datasets will be given as .npy files, and will contain data in a single numpy array corresponding to the unlabelled data X of shape num_samples by num_dimensions\n",
    "\n",
    "10. All plots must be labelled properly, all tables must have rows and columns named properly.\n",
    "\n",
    "11. Before subbmission ensure that you submit with the outputs (do not clear the outputs), so that when evaluating we can run selectively.\n",
    "\n",
    "12. Before submission ensure that the path for the folder containing the data is \"../../Data/\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans # This will be commented out during evaluation. Write your own k-means code.\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.datasets import load_digits\n",
    "from matplotlib.patches import Ellipse\n",
    "\n",
    "def practical_eigen_symmetric(L):\n",
    "    # Returns the eigen values and eigen vectors of a symmetric matrix L. eigen values are sorted in ascending order, and eig_vecs[:,i] corresponds to the ith eigen vector\n",
    "    eig_vals, eig_vecs = np.linalg.eigh(L)\n",
    "    eig_vecs = np.array(eig_vecs, dtype=np.float16)\n",
    "    eig_vecs = np.array(eig_vecs, dtype=np.float32)\n",
    "    return eig_vals, eig_vecs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1 Spectral Clustering.\n",
    "\n",
    "Write code for doing spectral clustering below. \n",
    "\n",
    "i.e. Convert the unlabelled data (Data A) into an adjacency matrix using D_{i,j} = exp(-\\gamma * ||x_i - x_j||). Convert the adjacency matrix into a Laplacian and find the lowest n eigen-vectors and use that to create feature matrix of shape num_samples-by-n. Use k-means clustering to cluster the resulting data.  \n",
    "\n",
    "Now plot the following scatterplots of the data with clusterlabels as colors.\n",
    "\n",
    "\n",
    "1. The results of k-means clustering on the raw data with k=3.\n",
    "2. The results of spectral clustering with k-means on the eigen features with gamma, n, k set to 10,3 and 3.\n",
    "3. The results of spectral clustering with k-means on the eigen features with gamma, n, k set to 10,10 and 3.\n",
    "4. The results of spectral clustering with k-means on the eigen features with gamma, n, k set to 1, 3 and 3.\n",
    "5. The results of spectral clustering with k-means on the eigen features with gamma, n, k set to 1, 10 and 3.\n",
    "\n",
    "\n",
    "Comment on the nature of the results in the text cell below.\n",
    "\n",
    "You are only allowed to use the pratical eigen vector finder given as defined above here. This is meant to simulate real eigen solvers which are iterative and approximate in nature. You can use the import of KMeans from sklearn to begin with, but the final submission should be based on your own implementation of kMeans or there will be a penalty.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codewrite cell (Use as you wish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codewrite cell (Get the 5 scatter plots here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Textwrite cell "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2: DB Scan\n",
    "\n",
    "Cluster the dataset B using DBScan. You may use the inbuilt DBscan function in sklearn  Explore the eps and min_samples parameter. \n",
    "\n",
    "Give a scatterplot of the datapoints that are not labelled as outlier by DBScan. Color the data points based on cluster label. \n",
    "\n",
    "Use eps values (0.1, 0.3 and 1) and min_samples values (3,10,30) for a total of 9 plots. Summarise your conclusions in the textwrite cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codewrite cell (Use as you wish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codewrite cell. Get the 9 plots here."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Textwrite cell"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3: PCA and k-Nearest Neighbours\n",
    "\n",
    "Consider the Digits dataset that is a part of the sklearn library. It consists of 1797 64 dimensional vectors with each corresponding to an 8x8 image of a digit. The label also gives the digit id. It is a 10-class classification problem.\n",
    "\n",
    "Choose a random subset of size 1500 for train and the rest for testing. Run k-Nearest neighbours with k values 1,3,7,15 and 31 and report the training and test accuracy. \n",
    "\n",
    "Repeat the above after performing PCA on the data. Use top n-principal components for n=2,4,8,16,32. For each n in the list report the best k-NN test accuracy and the k which achieves that accuracy and the approximation error for this particular value of n.\n",
    "\n",
    "Repeat the above for a noisy version of the data. i.e. add a random Gaussian noise of mean zero and variance 1 to all the 1797*64 input numbers.\n",
    "\n",
    "In total, the results should be given in 4 tables in the last textwrite cell:. Summarise your findings in a paragraph.\n",
    "\n",
    "Table 1: Raw data , k-NN performance. One row for each k.\n",
    "\n",
    "Table 2: n-component PCA preprocessed data k-NN performance. One row for each n.\n",
    "\n",
    "Table 3: Raw noised data, k-NN performance. One row for each k.\n",
    "\n",
    "Table 4: n-component PCA preprocessed noised data k-NN performance. One row for each n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codewrite cell (Use as you wish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codewrite cell (Do the experiments for filling Tables 1 and 2 here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codewrite cell (Do the experiments for filling Tables 3 and 4 here)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Textwrite cell"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4 : Expectation Maximisation for GMM.\n",
    "\n",
    "Use Dataset C for learning the parameters of a GMM using Expectation maximization. Set the number of compoents as 3. \n",
    "\n",
    "Plot the contours of the estimated components at init, and after 1 EM iteration, 2 EM iterations, 4 EM iterations and 8 EM iterations. (A single elliptical contour for each component is sufficient.) Add the scatterplot of the data points too here. \n",
    "\n",
    "Also, estimate plot the log likelihood of the data and the lower bound L(q,\\theta) thatis optimised as a function of the EM iteration number.\n",
    "\n",
    "Expected output: 4 plots giving the component means and variances along with the scatterplot. One plot giving iterations vs L(q;theta) and ln P(X|theta).\n",
    "\n",
    "You may use the code below for plotting the elliptical contours of the estimated components. It requires you to pass the axes of the plot figure as an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code read\n",
    "def plot_ellipses_gmm(means, covariances,ax):\n",
    "    color_list = ['r','g','b','k']\n",
    "    for i,(mean,cov) in enumerate(zip(means,covariances)):\n",
    "        \n",
    "        v1,w1=np.linalg.eigh(cov)\n",
    "        u = w1[0] / np.linalg.norm(w1[0])\n",
    "        angle = np.arctan2(u[1], u[0])\n",
    "        angle = 180 * angle / np.pi  # convert to degrees\n",
    "        v1 *= 3\n",
    "        ell = Ellipse(xy=mean, width=v1[0], height=v1[1], angle=180 + angle,\n",
    "                  edgecolor=color_list[i], lw=4, facecolor='none')\n",
    "        ax.add_artist(ell)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codewrite cell (Use as you wish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code write: Get the four plots with component contours here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code write: Get the plot of iterations vs L(q;theta) and ln P(X|theta) here. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "00ad5f1807eee938f7727b558c9158a01118eae9a3a444b82c1137c2e4c2794d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
